{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3613jvsc74a57bd06555959b910bb7a8fb6f47a2dcc8365b919d4ffbc6f566ad680344e31443b77e",
   "display_name": "Python 3.6.13 64-bit ('nn-env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Modeling Notebook with Advanced NLP Techniques"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing, utils\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models, layers, optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence, TaggedDocument\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open(\"../pickle/train.pickle\", \"rb\"))\n",
    "val = pickle.load(open(\"../pickle/val.pickle\", \"rb\"))\n",
    "test = pickle.load(open(\"../pickle/test.pickle\", \"rb\"))"
   ]
  },
  {
   "source": [
    "train.head()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               tweet  target  \\\n",
       "0  [reject, constantly, house, threaten, rape, mo...       1   \n",
       "1  [convince, lame, nigger, liver, believe, cuz, ...       1   \n",
       "2  [peace, fag, remember, best, lux, support, dro...       1   \n",
       "3                    [haha, ight, nig, calm, yoself]       1   \n",
       "4  [tits, better, look, face, make, like, asian, ...       1   \n",
       "\n",
       "                                              tweet2  \n",
       "0  'reject', 'constantly', 'house', 'threaten', '...  \n",
       "1  'convince', 'lame', 'nigger', 'liver', 'believ...  \n",
       "2  'peace', 'fag', 'remember', 'best', 'lux', 'su...  \n",
       "3            'haha', 'ight', 'nig', 'calm', 'yoself'  \n",
       "4  'tits', 'better', 'look', 'face', 'make', 'lik...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>target</th>\n      <th>tweet2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[reject, constantly, house, threaten, rape, mo...</td>\n      <td>1</td>\n      <td>'reject', 'constantly', 'house', 'threaten', '...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[convince, lame, nigger, liver, believe, cuz, ...</td>\n      <td>1</td>\n      <td>'convince', 'lame', 'nigger', 'liver', 'believ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[peace, fag, remember, best, lux, support, dro...</td>\n      <td>1</td>\n      <td>'peace', 'fag', 'remember', 'best', 'lux', 'su...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[haha, ight, nig, calm, yoself]</td>\n      <td>1</td>\n      <td>'haha', 'ight', 'nig', 'calm', 'yoself'</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[tits, better, look, face, make, like, asian, ...</td>\n      <td>1</td>\n      <td>'tits', 'better', 'look', 'face', 'make', 'lik...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               tweet  target  \\\n",
       "0  [lbum, fotos, gaywrites, make, project, queer,...       1   \n",
       "1  [yay, america, israel, jew, hat, muslim, trash...       1   \n",
       "2  [miss, ofay, friends, day, scar, recent, happe...       1   \n",
       "3     [trash, darkskin, nigga, steal, damn, garbage]       1   \n",
       "4      [cody, call, people, nigger, hes, fuck, spaz]       1   \n",
       "\n",
       "                                              tweet2  \n",
       "0  'lbum', 'fotos', 'gaywrites', 'make', 'project...  \n",
       "1  'yay', 'america', 'israel', 'jew', 'hat', 'mus...  \n",
       "2  'miss', 'ofay', 'friends', 'day', 'scar', 'rec...  \n",
       "3  'trash', 'darkskin', 'nigga', 'steal', 'damn',...  \n",
       "4  'cody', 'call', 'people', 'nigger', 'hes', 'fu...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>target</th>\n      <th>tweet2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[lbum, fotos, gaywrites, make, project, queer,...</td>\n      <td>1</td>\n      <td>'lbum', 'fotos', 'gaywrites', 'make', 'project...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[yay, america, israel, jew, hat, muslim, trash...</td>\n      <td>1</td>\n      <td>'yay', 'america', 'israel', 'jew', 'hat', 'mus...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[miss, ofay, friends, day, scar, recent, happe...</td>\n      <td>1</td>\n      <td>'miss', 'ofay', 'friends', 'day', 'scar', 'rec...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[trash, darkskin, nigga, steal, damn, garbage]</td>\n      <td>1</td>\n      <td>'trash', 'darkskin', 'nigga', 'steal', 'damn',...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[cody, call, people, nigger, hes, fuck, spaz]</td>\n      <td>1</td>\n      <td>'cody', 'call', 'people', 'nigger', 'hes', 'fu...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target2'] = np.nan\n",
    "train.loc[train.target == 1, 'target2'] = 'Hate'\n",
    "train.loc[train.target == 0, 'target2'] = 'Not_Hate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "val['target2'] = np.nan\n",
    "val.loc[val.target == 1, 'target2'] = 'Hate'\n",
    "val.loc[val.target == 0, 'target2'] = 'Not_Hate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = train.tweet\n",
    "X_val = val.tweet\n",
    "y_tr = train.target\n",
    "y_val = val.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0        [reject, constantly, house, threaten, rape, mo...\n",
       "1        [convince, lame, nigger, liver, believe, cuz, ...\n",
       "2        [peace, fag, remember, best, lux, support, dro...\n",
       "3                          [haha, ight, nig, calm, yoself]\n",
       "4        [tits, better, look, face, make, like, asian, ...\n",
       "                               ...                        \n",
       "18581                                   [miss, lil, bitch]\n",
       "18582          [gotta, hoe, smh, aint, captain, save, hoe]\n",
       "18583                  [lmao, yeah, bitch, lil, shit, rip]\n",
       "18584                                    [tbt, bad, bitch]\n",
       "18585                          [hoe, act, know, imma, let]\n",
       "Name: tweet, Length: 18586, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "X_tr"
   ]
  },
  {
   "source": [
    "# Doc2Vec\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## DBOW (Distributed Bag of Words)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TaggedDocument(words=['retard', 'bruh', 'lol'], tags='Hate')"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "\n",
    "train_tagged = train.apply(lambda x: TaggedDocument(words=x['tweet'], tags=x.target2), axis=1)\n",
    "train_tagged.values[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TaggedDocument(words=['fucc', 'nicca', 'pose', 'pullin'], tags='Hate')"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "val_tagged = val.apply(lambda x: TaggedDocument(words=x['tweet'], tags=x.target2), axis=1)\n",
    "val_tagged.values[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 18586/18586 [00:00<00:00, 961604.26it/s]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 18586/18586 [00:00<00:00, 1995120.27it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2923178.87it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3134764.92it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2819463.06it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2862532.01it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2653708.27it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2613319.95it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3047391.98it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2532908.80it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3114973.79it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2907587.71it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2841869.93it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3188357.22it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3045368.16it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3189270.31it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2825902.06it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3099738.92it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2818138.03it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2932856.82it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2846227.83it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2831033.34it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2950729.93it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2931533.32it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2602414.76it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2785611.37it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2634783.32it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2748196.23it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3093465.64it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2550226.84it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3006955.99it/s]\n",
      "CPU times: user 48 s, sys: 4.44 s, total: 52.5 s\n",
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr, X_tr = vec_for_learning(model_dbow, train_tagged)\n",
    "y_val, X_val = vec_for_learning(model_dbow, val_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, n_jobs=1)"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9418886198547215"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "logreg.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9418886198547215"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.save('../data/d2v_model_dbow.doc2vec')\n",
    "model_dbow = Doc2Vec.load('../data/d2v_model_dbow.doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "source": [
    "## DMM (Distributed Memory Mean)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 18586/18586 [00:00<00:00, 1954600.56it/s]\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=100, window=10, negative=5, min_count=1, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 18586/18586 [00:00<00:00, 2076427.94it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2755579.15it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3265828.83it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3102329.44it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3038838.89it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2759969.34it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3041921.96it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3170075.81it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3118837.13it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2825902.06it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2849661.29it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2995286.80it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3290643.06it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3121584.68it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3144755.10it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2391708.11it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2946825.97it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2611831.48it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3211077.73it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3129479.49it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2957670.98it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2725996.93it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2577461.87it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2853625.23it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 3305993.81it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2928229.82it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2787005.62it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2883816.74it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2761240.23it/s]\n",
      "100%|██████████| 18586/18586 [00:00<00:00, 2927899.87it/s]\n",
      "CPU times: user 52.6 s, sys: 17.6 s, total: 1min 10s\n",
      "Wall time: 53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dmm.alpha -= 0.002\n",
    "    model_dmm.min_alpha = model_dmm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dmm.save('../data/d2v_model_dmm.doc2vec')\n",
    "model_dmm = Doc2Vec.load('../data/d2v_model_dmm.doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('niggers', 0.6091761589050293),\n",
       " ('scully', 0.5996459722518921),\n",
       " ('strength', 0.5672992467880249),\n",
       " ('mariners', 0.5286785364151001),\n",
       " ('vin', 0.5232475996017456),\n",
       " ('bedroom', 0.5231317281723022),\n",
       " ('tbird', 0.49917277693748474),\n",
       " ('unroll', 0.49384060502052307),\n",
       " ('devil', 0.49172112345695496),\n",
       " ('uncalled', 0.4915260970592499)]"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "model_dmm.most_similar('nigger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr, X_tr = vec_for_learning(model_dmm, train_tagged)\n",
    "y_val, X_val = vec_for_learning(model_dmm, val_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "logreg = LogisticRegression().fit(X_tr, y_tr)\n",
    "logreg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9174065106268496"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "clf.score(validation_vecs_dmm, y_val)"
   ]
  },
  {
   "source": [
    "# ANN with Tfidf Vectorizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=100000, ngram_range=(1, 3))"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "\n",
    "tvec1 = TfidfVectorizer(max_features=100000,ngram_range=(1, 3))\n",
    "tvec1.fit(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf = tvec1.transform(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation_tfidf = tvec1.transform(X_val).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 3.95 s, sys: 2.09 s, total: 6.04 s\nWall time: 1.81 s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train_tfidf, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.944578961528114"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "clf.score(x_validation_tfidf, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X_data, y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].toarray()\n",
    "        y_batch = y_data[y_data.index[index_batch]]\n",
    "        counter += 1\n",
    "        yield np.array(X_batch), np.array(y_batch)\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 23s 38ms/step - loss: 0.5717 - accuracy: 0.7947 - val_loss: 0.3167 - val_accuracy: 0.9424\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.8485 - accuracy: 0.7785 - val_loss: 0.3863 - val_accuracy: 0.9424\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 23s 39ms/step - loss: 0.7411 - accuracy: 0.7924 - val_loss: 0.3191 - val_accuracy: 0.9424\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 22s 37ms/step - loss: 0.3069 - accuracy: 0.8735 - val_loss: 0.2377 - val_accuracy: 0.9432\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 23s 39ms/step - loss: 0.0923 - accuracy: 0.9696 - val_loss: 0.2436 - val_accuracy: 0.9422\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.0611 - accuracy: 0.9816 - val_loss: 0.2560 - val_accuracy: 0.9414\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 21s 37ms/step - loss: 0.0481 - accuracy: 0.9845 - val_loss: 0.2701 - val_accuracy: 0.9405\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 21s 35ms/step - loss: 0.0406 - accuracy: 0.9865 - val_loss: 0.2847 - val_accuracy: 0.9424\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 23s 40ms/step - loss: 0.0359 - accuracy: 0.9859 - val_loss: 0.3016 - val_accuracy: 0.9430\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 22s 38ms/step - loss: 0.0329 - accuracy: 0.9861 - val_loss: 0.3142 - val_accuracy: 0.9424\n",
      "CPU times: user 16min 6s, sys: 1min 52s, total: 17min 58s\n",
      "Wall time: 3min 39s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f825d08a128>"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "%%time\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation = 'relu', input_dim = 100000))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.fit_generator(generator = batch_generator(x_train_tfidf, y_tr, 32),\n",
    "                    epochs = 10, \n",
    "                    random_state = 42, \n",
    "                    validation_data = (x_validation_tfidf, y_val),\n",
    "                    steps_per_epoch = x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "source": [
    "## Normalizing Inputs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "norm = Normalizer().fit(x_train_tfidf)\n",
    "x_train_tfidf_norm = norm.transform(x_train_tfidf)\n",
    "x_validation_tfidf_norm = norm.transform(x_validation_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 23s 38ms/step - loss: 0.5787 - accuracy: 0.7866 - val_loss: 0.3237 - val_accuracy: 0.9424\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.8758 - accuracy: 0.7789 - val_loss: 0.4325 - val_accuracy: 0.9424\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.8750 - accuracy: 0.7815 - val_loss: 0.3066 - val_accuracy: 0.9424\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 24s 40ms/step - loss: 0.3163 - accuracy: 0.8620 - val_loss: 0.2680 - val_accuracy: 0.9430\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.1300 - accuracy: 0.9497 - val_loss: 0.2711 - val_accuracy: 0.9430\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 21s 37ms/step - loss: 0.0793 - accuracy: 0.9748 - val_loss: 0.2628 - val_accuracy: 0.9416\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.0498 - accuracy: 0.9836 - val_loss: 0.2745 - val_accuracy: 0.9405\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 21s 37ms/step - loss: 0.0415 - accuracy: 0.9860 - val_loss: 0.2880 - val_accuracy: 0.9422\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 20s 35ms/step - loss: 0.0366 - accuracy: 0.9863 - val_loss: 0.3019 - val_accuracy: 0.9424\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.0334 - accuracy: 0.9862 - val_loss: 0.3279 - val_accuracy: 0.9424\n",
      "CPU times: user 16min 14s, sys: 1min 56s, total: 18min 10s\n",
      "Wall time: 3min 33s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f80cfab0f98>"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "%%time\n",
    "model_n = Sequential()\n",
    "model_n.add(Dense(64, activation = 'relu', input_dim = 100000))\n",
    "model_n.add(Dense(1, activation ='sigmoid'))\n",
    "model_n.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model_n.fit_generator(generator = batch_generator(x_train_tfidf_norm, y_tr, 32),\n",
    "                      epochs = 10, \n",
    "                      validation_data = (x_validation_tfidf_norm, y_val),\n",
    "                      steps_per_epoch = x_train_tfidf_norm.shape[0]/32)"
   ]
  },
  {
   "source": [
    "## Using Dropout for Overfitting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 26s 44ms/step - loss: 0.5739 - accuracy: 0.7964 - val_loss: 0.3167 - val_accuracy: 0.9424\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.8738 - accuracy: 0.7787 - val_loss: 0.3830 - val_accuracy: 0.9424\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 22s 37ms/step - loss: 0.8162 - accuracy: 0.7830 - val_loss: 0.2810 - val_accuracy: 0.9424\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 24s 40ms/step - loss: 0.3484 - accuracy: 0.8593 - val_loss: 0.2562 - val_accuracy: 0.9430\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 22s 38ms/step - loss: 0.1655 - accuracy: 0.9331 - val_loss: 0.2608 - val_accuracy: 0.9422\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.1039 - accuracy: 0.9638 - val_loss: 0.2659 - val_accuracy: 0.9424\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 22s 37ms/step - loss: 0.0707 - accuracy: 0.9769 - val_loss: 0.2715 - val_accuracy: 0.9414\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.0531 - accuracy: 0.9822 - val_loss: 0.2824 - val_accuracy: 0.9403\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 20s 35ms/step - loss: 0.0453 - accuracy: 0.9837 - val_loss: 0.3016 - val_accuracy: 0.9419\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.0399 - accuracy: 0.9865 - val_loss: 0.3129 - val_accuracy: 0.9414\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f80cf965eb8>"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.fit_generator(generator=batch_generator(x_train_tfidf, y_tr, 32),\n",
    "                    epochs=10, validation_data=(x_validation_tfidf, y_val),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "source": [
    "## Shuffling Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator_shuffle(X_data, y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    np.random.shuffle(index)\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].toarray()\n",
    "        y_batch = y_data[y_data.index[index_batch]]\n",
    "        counter += 1\n",
    "        yield np.array(X_batch), np.array(y_batch)\n",
    "        if (counter > number_of_batches):\n",
    "            np.random.shuffle(index)\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 25s 40ms/step - loss: 0.3838 - accuracy: 0.9369 - val_loss: 0.1864 - val_accuracy: 0.9414\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 23s 39ms/step - loss: 0.1334 - accuracy: 0.9441 - val_loss: 0.1732 - val_accuracy: 0.9430\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 24s 42ms/step - loss: 0.0648 - accuracy: 0.9769 - val_loss: 0.1909 - val_accuracy: 0.9381\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 25s 43ms/step - loss: 0.0299 - accuracy: 0.9927 - val_loss: 0.2163 - val_accuracy: 0.9397\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 23s 39ms/step - loss: 0.0198 - accuracy: 0.9950 - val_loss: 0.2417 - val_accuracy: 0.9392\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.0166 - accuracy: 0.9955 - val_loss: 0.2558 - val_accuracy: 0.9379\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 23s 40ms/step - loss: 0.0118 - accuracy: 0.9965 - val_loss: 0.2735 - val_accuracy: 0.9365\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 24s 41ms/step - loss: 0.0100 - accuracy: 0.9960 - val_loss: 0.2871 - val_accuracy: 0.9368\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 27s 47ms/step - loss: 0.0095 - accuracy: 0.9959 - val_loss: 0.3062 - val_accuracy: 0.9357\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 23s 40ms/step - loss: 0.0077 - accuracy: 0.9969 - val_loss: 0.3177 - val_accuracy: 0.9346\n",
      "CPU times: user 15min 44s, sys: 1min 52s, total: 17min 37s\n",
      "Wall time: 3min 58s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8013777cf8>"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "%%time\n",
    "model_s = Sequential()\n",
    "model_s.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model_s.add(Dense(1, activation='sigmoid'))\n",
    "model_s.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_s.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_tr, 32),\n",
    "                    epochs=10, validation_data=(x_validation_tfidf, y_val),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "source": [
    "## Shuffle and Dropout"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 29s 49ms/step - loss: 0.3737 - accuracy: 0.9375 - val_loss: 0.1844 - val_accuracy: 0.9416\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 23s 39ms/step - loss: 0.1436 - accuracy: 0.9445 - val_loss: 0.1730 - val_accuracy: 0.9430\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 24s 41ms/step - loss: 0.0717 - accuracy: 0.9730 - val_loss: 0.1895 - val_accuracy: 0.9392\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 24s 41ms/step - loss: 0.0349 - accuracy: 0.9903 - val_loss: 0.2123 - val_accuracy: 0.9381\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 23s 40ms/step - loss: 0.0222 - accuracy: 0.9943 - val_loss: 0.2310 - val_accuracy: 0.9370\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 24s 41ms/step - loss: 0.0169 - accuracy: 0.9946 - val_loss: 0.2546 - val_accuracy: 0.9370\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 23s 40ms/step - loss: 0.0132 - accuracy: 0.9957 - val_loss: 0.2672 - val_accuracy: 0.9368\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 23s 39ms/step - loss: 0.0110 - accuracy: 0.9955 - val_loss: 0.2842 - val_accuracy: 0.9373\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 23s 40ms/step - loss: 0.0098 - accuracy: 0.9962 - val_loss: 0.3014 - val_accuracy: 0.9373\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 22s 38ms/step - loss: 0.0082 - accuracy: 0.9970 - val_loss: 0.3092 - val_accuracy: 0.9354\n",
      "CPU times: user 16min 3s, sys: 1min 59s, total: 18min 2s\n",
      "Wall time: 3min 58s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7fbf528518>"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "%%time\n",
    "model_s_1 = Sequential()\n",
    "model_s_1.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model_s_1.add(Dropout(0.2))\n",
    "model_s_1.add(Dense(1, activation='sigmoid'))\n",
    "model_s_1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_s_1.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_tr, 32),\n",
    "                    epochs=10, validation_data=(x_validation_tfidf, y_val),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "source": [
    "## Learning Rate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 37s 62ms/step - loss: 0.2616 - accuracy: 0.9293 - val_loss: 0.1741 - val_accuracy: 0.9424\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 28s 48ms/step - loss: 0.0662 - accuracy: 0.9629 - val_loss: 0.2558 - val_accuracy: 0.9292\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 26s 45ms/step - loss: 0.0250 - accuracy: 0.9921 - val_loss: 0.2860 - val_accuracy: 0.9244\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 27s 47ms/step - loss: 0.0134 - accuracy: 0.9953 - val_loss: 0.3190 - val_accuracy: 0.9341\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 32s 54ms/step - loss: 0.0114 - accuracy: 0.9958 - val_loss: 0.3377 - val_accuracy: 0.9319\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 29s 51ms/step - loss: 0.0097 - accuracy: 0.9964 - val_loss: 0.3698 - val_accuracy: 0.9349\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 30s 51ms/step - loss: 0.0097 - accuracy: 0.9956 - val_loss: 0.3596 - val_accuracy: 0.9290\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 32s 54ms/step - loss: 0.0096 - accuracy: 0.9951 - val_loss: 0.3870 - val_accuracy: 0.9338\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 36s 62ms/step - loss: 0.0086 - accuracy: 0.9956 - val_loss: 0.3872 - val_accuracy: 0.9327\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 31s 54ms/step - loss: 0.0074 - accuracy: 0.9959 - val_loss: 0.3735 - val_accuracy: 0.9271\n",
      "CPU times: user 16min 32s, sys: 2min 8s, total: 18min 41s\n",
      "Wall time: 5min 8s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7efc4f9cf8>"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "custom_adam = keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model_testing_2 = Sequential()\n",
    "model_testing_2.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model_testing_2.add(Dense(1, activation='sigmoid'))\n",
    "model_testing_2.compile(optimizer=custom_adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_testing_2.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_tr, 32),\n",
    "                    epochs=10, validation_data=(x_validation_tfidf, y_val),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 41s 68ms/step - loss: 0.2273 - accuracy: 0.9414 - val_loss: 0.1879 - val_accuracy: 0.9424\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 31s 53ms/step - loss: 0.0512 - accuracy: 0.9808 - val_loss: 0.3134 - val_accuracy: 0.9338\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 28s 47ms/step - loss: 0.0187 - accuracy: 0.9926 - val_loss: 0.3171 - val_accuracy: 0.9322\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 28s 49ms/step - loss: 0.0140 - accuracy: 0.9937 - val_loss: 0.3170 - val_accuracy: 0.9190\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 28s 48ms/step - loss: 0.0110 - accuracy: 0.9949 - val_loss: 0.4019 - val_accuracy: 0.9384\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 29s 50ms/step - loss: 0.0084 - accuracy: 0.9960 - val_loss: 0.3401 - val_accuracy: 0.9252\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 29s 49ms/step - loss: 0.0072 - accuracy: 0.9965 - val_loss: 0.3432 - val_accuracy: 0.9247\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 28s 49ms/step - loss: 0.0068 - accuracy: 0.9973 - val_loss: 0.3478 - val_accuracy: 0.9163\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 29s 49ms/step - loss: 0.0072 - accuracy: 0.9969 - val_loss: 0.3670 - val_accuracy: 0.9131\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 28s 48ms/step - loss: 0.0074 - accuracy: 0.9969 - val_loss: 0.3785 - val_accuracy: 0.9252\n",
      "CPU times: user 16min 22s, sys: 2min 9s, total: 18min 31s\n",
      "Wall time: 4min 58s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7fbf53ae10>"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "%%time\n",
    "custom_adam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model_testing_3 = Sequential()\n",
    "model_testing_3.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model_testing_3.add(Dense(1, activation='sigmoid'))\n",
    "model_testing_3.compile(optimizer=custom_adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_testing_3.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_tr, 32),\n",
    "                    epochs=10, validation_data=(x_validation_tfidf, y_val),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 42s 70ms/step - loss: 0.2224 - accuracy: 0.9356 - val_loss: 0.2158 - val_accuracy: 0.9405\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 31s 53ms/step - loss: 0.0872 - accuracy: 0.9680 - val_loss: 0.3700 - val_accuracy: 0.9368\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 27s 46ms/step - loss: 0.0549 - accuracy: 0.9832 - val_loss: 0.5258 - val_accuracy: 0.9349\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 34s 58ms/step - loss: 0.0346 - accuracy: 0.9886 - val_loss: 0.3928 - val_accuracy: 0.9354\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 28s 47ms/step - loss: 0.0260 - accuracy: 0.9903 - val_loss: 0.4796 - val_accuracy: 0.9349\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 40s 68ms/step - loss: 0.0249 - accuracy: 0.9920 - val_loss: 0.5224 - val_accuracy: 0.9327\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 37s 63ms/step - loss: 0.0242 - accuracy: 0.9918 - val_loss: 0.5153 - val_accuracy: 0.9319\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 35s 61ms/step - loss: 0.0178 - accuracy: 0.9938 - val_loss: 0.4306 - val_accuracy: 0.9322\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 35s 60ms/step - loss: 0.0221 - accuracy: 0.9913 - val_loss: 0.4697 - val_accuracy: 0.9284\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 34s 58ms/step - loss: 0.0195 - accuracy: 0.9933 - val_loss: 0.3720 - val_accuracy: 0.9284\n",
      "CPU times: user 16min 38s, sys: 2min 21s, total: 19min\n",
      "Wall time: 5min 42s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7f004954a8>"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "%%time\n",
    "custom_adam = keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model_testing_4 = Sequential()\n",
    "model_testing_4.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model_testing_4.add(Dense(1, activation='sigmoid'))\n",
    "model_testing_4.compile(optimizer=custom_adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_testing_4.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_tr, 32),\n",
    "                              epochs=10, \n",
    "                              validation_data=(x_validation_tfidf, y_val),\n",
    "                              steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 59s 97ms/step - loss: 0.4546 - accuracy: 0.9351 - val_loss: 0.1989 - val_accuracy: 0.9424\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 36s 61ms/step - loss: 0.1671 - accuracy: 0.9451 - val_loss: 0.1752 - val_accuracy: 0.9430\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 44s 76ms/step - loss: 0.1145 - accuracy: 0.9497 - val_loss: 0.1758 - val_accuracy: 0.9435\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 31s 53ms/step - loss: 0.0658 - accuracy: 0.9778 - val_loss: 0.1864 - val_accuracy: 0.9400\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 33s 58ms/step - loss: 0.0375 - accuracy: 0.9899 - val_loss: 0.1997 - val_accuracy: 0.9384\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 34s 59ms/step - loss: 0.0262 - accuracy: 0.9928 - val_loss: 0.2136 - val_accuracy: 0.9376\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 38s 65ms/step - loss: 0.0166 - accuracy: 0.9957 - val_loss: 0.2261 - val_accuracy: 0.9376\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 34s 58ms/step - loss: 0.0135 - accuracy: 0.9963 - val_loss: 0.2392 - val_accuracy: 0.9381\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 34s 58ms/step - loss: 0.0115 - accuracy: 0.9961 - val_loss: 0.2509 - val_accuracy: 0.9376\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 36s 61ms/step - loss: 0.0101 - accuracy: 0.9962 - val_loss: 0.2630 - val_accuracy: 0.9362\n",
      "CPU times: user 17min 4s, sys: 2min 30s, total: 19min 34s\n",
      "Wall time: 6min 19s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7fb38cf898>"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "%%time\n",
    "custom_adam = keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model_testing_5 = Sequential()\n",
    "model_testing_5.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model_testing_5.add(Dense(1, activation='sigmoid'))\n",
    "model_testing_5.compile(optimizer=custom_adam,\n",
    "                        loss='binary_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "model_testing_5.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_tr, 32),\n",
    "                              epochs=10, \n",
    "                              validation_data=(x_validation_tfidf, y_val),\n",
    "                              steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "source": [
    "## Increasing number of hidden nodes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 70s 113ms/step - loss: 0.3306 - accuracy: 0.9425 - val_loss: 0.1761 - val_accuracy: 0.9424\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 46s 79ms/step - loss: 0.1100 - accuracy: 0.9463 - val_loss: 0.1859 - val_accuracy: 0.9387\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 46s 79ms/step - loss: 0.0399 - accuracy: 0.9884 - val_loss: 0.2260 - val_accuracy: 0.9376\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 43s 74ms/step - loss: 0.0197 - accuracy: 0.9942 - val_loss: 0.2539 - val_accuracy: 0.9360\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 53s 92ms/step - loss: 0.0135 - accuracy: 0.9959 - val_loss: 0.2730 - val_accuracy: 0.9357\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 64s 110ms/step - loss: 0.0100 - accuracy: 0.9966 - val_loss: 0.2904 - val_accuracy: 0.9341\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 60s 103ms/step - loss: 0.0082 - accuracy: 0.9963 - val_loss: 0.3066 - val_accuracy: 0.9352\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 65s 112ms/step - loss: 0.0093 - accuracy: 0.9962 - val_loss: 0.3224 - val_accuracy: 0.9352\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 58s 100ms/step - loss: 0.0078 - accuracy: 0.9963 - val_loss: 0.3255 - val_accuracy: 0.9322\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 51s 88ms/step - loss: 0.0064 - accuracy: 0.9973 - val_loss: 0.3399 - val_accuracy: 0.9322\n",
      "CPU times: user 29min 22s, sys: 2min 55s, total: 32min 18s\n",
      "Wall time: 9min 18s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7ef5128cc0>"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "%%time\n",
    "model_s_2 = Sequential()\n",
    "model_s_2.add(Dense(128, activation='relu', input_dim=100000))\n",
    "model_s_2.add(Dense(1, activation='sigmoid'))\n",
    "model_s_2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_s_2.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_tr, 32),\n",
    "                        epochs=10, \n",
    "                        validation_data=(x_validation_tfidf, y_val),\n",
    "                        steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}