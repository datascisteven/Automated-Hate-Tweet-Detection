{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3613jvsc74a57bd06555959b910bb7a8fb6f47a2dcc8365b919d4ffbc6f566ad680344e31443b77e",
   "display_name": "Python 3.6.13 64-bit ('nn-env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "import multiprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open(\"../pickle/train.pickle\", \"rb\"))\n",
    "val = pickle.load(open(\"../pickle/val.pickle\", \"rb\"))\n",
    "test = pickle.load(open(\"../pickle/test.pickle\", \"rb\"))"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = train.tweet\n",
    "X_val = val.tweet\n",
    "X_tt = test.tweet\n",
    "y_tr = train.target\n",
    "y_val = val.target\n",
    "y_tt = test.target"
   ]
  },
  {
   "source": [
    "# Doc2Vec\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize_tweets_ug(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(tweets.index, tweets):\n",
    "        result.append(LabeledSentence(t.split(), [prefix + '_%s' % i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/examsherpa/opt/anaconda3/envs/nn-env/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "all_x = pd.concat([X_tr, X_val, X_tt])\n",
    "all_x_w2v = labelize_tweets_ug(all_x, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "24783"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "len(all_x_w2v)"
   ]
  },
  {
   "source": [
    "## DBOW (Distributed Bag of Words)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/examsherpa/opt/anaconda3/envs/nn-env/lib/python3.6/site-packages/gensim/models/doc2vec.py:319: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2386688.31it/s]\n"
     ]
    }
   ],
   "source": [
    "model_ug_dbow = Doc2Vec(dm=0, size=100, negative=5, min_count=2, workers=4, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dbow.build_vocab([x for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 24783/24783 [00:00<00:00, 2748114.63it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3167487.46it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3303484.27it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3373711.86it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3328341.58it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2543118.76it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2466716.56it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2971200.12it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2778377.46it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2581262.38it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2695242.98it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2827886.07it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2687647.02it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2388498.07it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2247074.86it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2854522.48it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2657686.54it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2220861.79it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2882784.29it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2122199.14it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2508384.07it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2445534.31it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 1789235.68it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2328362.96it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2511474.93it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 1407090.94it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2669974.21it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2625266.73it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2742169.94it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2334323.74it/s]\n",
      "CPU times: user 34.6 s, sys: 5.99 s, total: 40.6 s\n",
      "Wall time: 32.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_ug_dbow.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dbow.alpha -= 0.002\n",
    "    model_ug_dbow.min_alpha = model_ug_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = model.docvecs[prefix]\n",
    "        n += 1\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dbow = get_vectors(model_ug_dbow, X_tr, 100)\n",
    "validation_vecs_dbow = get_vectors(model_ug_dbow, X_val, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9031476997578692"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "clf.score(validation_vecs_dbow, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_dbow.save('../data/d2v_model_ug_dbow.doc2vec')\n",
    "model_ug_dbow = Doc2Vec.load('../data/d2v_model_ug_dbow.doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "source": [
    "## DMC (Distributed Memory Concatenated) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 24783/24783 [00:00<00:00, 2682099.19it/s]\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_dmc = Doc2Vec(dm=1, dm_concat=1, size=100, window=2, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dmc.build_vocab([x for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 24783/24783 [00:00<00:00, 2425901.10it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3418761.26it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3300861.71it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2704077.31it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3195825.99it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3389442.94it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3081293.49it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3501328.35it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3198677.91it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3065301.41it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3226477.82it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3557163.64it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3405767.70it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3332396.24it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3332396.24it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3516846.64it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3531303.03it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3482792.87it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3454665.69it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3296569.71it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3076369.12it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3092845.25it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3139078.22it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2995344.38it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2784331.18it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3230287.95it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3410796.56it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3338604.02it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2942935.82it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3483726.66it/s]\n",
      "CPU times: user 30.8 s, sys: 4.57 s, total: 35.3 s\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_ug_dmc.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dmc.alpha -= 0.002\n",
    "    model_ug_dmc.min_alpha = model_ug_dmc.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_dmc.save('../data/d2v_model_ug_dmc.doc2vec')\n",
    "model_ug_dmc = Doc2Vec.load('../data/d2v_model_ug_dmc.doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "\"word 'jew' not in vocabulary\"",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-093cb69a0437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_ug_dmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'jew'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/nn-env/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1459\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m                 )\n\u001b[0;32m-> 1461\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn-env/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m         \"\"\"\n\u001b[0;32m-> 1383\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.wmdistance() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn-env/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn-env/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'jew' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model_ug_dmc.most_similar('jew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dmc = get_vectors(model_ug_dmc, X_tr, 100)\n",
    "validation_vecs_dmc = get_vectors(model_ug_dmc, X_val, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmc, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9408124831853646"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "clf.score(validation_vecs_dmc, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_dmc.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "source": [
    "## DMM (Distributed Memory Mean)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 24783/24783 [00:00<00:00, 2293025.59it/s]\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_dmm = Doc2Vec(dm=1, dm_mean=1, size=100, window=4, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dmm.build_vocab([x for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 24783/24783 [00:00<00:00, 2419520.41it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2704992.09it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 2637389.59it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3378756.25it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3129627.15it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3593936.87it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3669941.96it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3332396.24it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3539961.72it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3405209.85it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3627043.37it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3317612.54it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3419773.52it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3398529.92it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3641273.55it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3394867.11it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3600285.26it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3702227.30it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3607532.31it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3183006.28it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3728922.23it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3441626.20it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3175615.92it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3853044.56it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3479761.52it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3476968.02it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3402757.50it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3799387.26it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3550481.13it/s]\n",
      "100%|██████████| 24783/24783 [00:00<00:00, 3548541.84it/s]\n",
      "CPU times: user 52.8 s, sys: 18.5 s, total: 1min 11s\n",
      "Wall time: 54.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_ug_dmm.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dmm.alpha -= 0.002\n",
    "    model_ug_dmm.min_alpha = model_ug_dmm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_dmm.save('../data/d2v_model_ug_dmm.doc2vec')\n",
    "model_ug_dmm = Doc2Vec.load('../data/d2v_model_ug_dmm.doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "\"word 'nigger' not in vocabulary\"",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-ba378a5696f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_ug_dmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nigger'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/nn-env/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1459\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m                 )\n\u001b[0;32m-> 1461\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn-env/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m         \"\"\"\n\u001b[0;32m-> 1383\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.wmdistance() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn-env/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn-env/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'nigger' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model_ug_dmm.most_similar('nigger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dmm = get_vectors(model_ug_dmm, X_tr, 100)\n",
    "validation_vecs_dmm = get_vectors(model_ug_dmm, X_val, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmm, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9174065106268496"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "clf.score(validation_vecs_dmm, y_val)"
   ]
  },
  {
   "source": [
    "# ANN with Tfidf Vectorizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=100000, ngram_range=(1, 3))"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec1 = TfidfVectorizer(max_features=100000,ngram_range=(1, 3))\n",
    "tvec1.fit(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf = tvec1.transform(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation_tfidf = tvec1.transform(X_val).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 3.95 s, sys: 2.09 s, total: 6.04 s\nWall time: 1.81 s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train_tfidf, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.944578961528114"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "clf.score(x_validation_tfidf, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X_data, y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].toarray()\n",
    "        y_batch = y_data[y_data.index[index_batch]]\n",
    "        counter += 1\n",
    "        yield np.array(X_batch), np.array(y_batch)\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 23s 38ms/step - loss: 0.5717 - accuracy: 0.7947 - val_loss: 0.3167 - val_accuracy: 0.9424\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.8485 - accuracy: 0.7785 - val_loss: 0.3863 - val_accuracy: 0.9424\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 23s 39ms/step - loss: 0.7411 - accuracy: 0.7924 - val_loss: 0.3191 - val_accuracy: 0.9424\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 22s 37ms/step - loss: 0.3069 - accuracy: 0.8735 - val_loss: 0.2377 - val_accuracy: 0.9432\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 23s 39ms/step - loss: 0.0923 - accuracy: 0.9696 - val_loss: 0.2436 - val_accuracy: 0.9422\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.0611 - accuracy: 0.9816 - val_loss: 0.2560 - val_accuracy: 0.9414\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 21s 37ms/step - loss: 0.0481 - accuracy: 0.9845 - val_loss: 0.2701 - val_accuracy: 0.9405\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 21s 35ms/step - loss: 0.0406 - accuracy: 0.9865 - val_loss: 0.2847 - val_accuracy: 0.9424\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 23s 40ms/step - loss: 0.0359 - accuracy: 0.9859 - val_loss: 0.3016 - val_accuracy: 0.9430\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 22s 38ms/step - loss: 0.0329 - accuracy: 0.9861 - val_loss: 0.3142 - val_accuracy: 0.9424\n",
      "CPU times: user 16min 6s, sys: 1min 52s, total: 17min 58s\n",
      "Wall time: 3min 39s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f825d08a128>"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "%%time\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation = 'relu', input_dim = 100000))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.fit_generator(generator = batch_generator(x_train_tfidf, y_tr, 32),\n",
    "                    epochs = 10, \n",
    "                    random_state = 42, \n",
    "                    validation_data = (x_validation_tfidf, y_val),\n",
    "                    steps_per_epoch = x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "source": [
    "## Normalizing Inputs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "norm = Normalizer().fit(x_train_tfidf)\n",
    "x_train_tfidf_norm = norm.transform(x_train_tfidf)\n",
    "x_validation_tfidf_norm = norm.transform(x_validation_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 23s 38ms/step - loss: 0.5787 - accuracy: 0.7866 - val_loss: 0.3237 - val_accuracy: 0.9424\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.8758 - accuracy: 0.7789 - val_loss: 0.4325 - val_accuracy: 0.9424\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.8750 - accuracy: 0.7815 - val_loss: 0.3066 - val_accuracy: 0.9424\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 24s 40ms/step - loss: 0.3163 - accuracy: 0.8620 - val_loss: 0.2680 - val_accuracy: 0.9430\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.1300 - accuracy: 0.9497 - val_loss: 0.2711 - val_accuracy: 0.9430\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 21s 37ms/step - loss: 0.0793 - accuracy: 0.9748 - val_loss: 0.2628 - val_accuracy: 0.9416\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.0498 - accuracy: 0.9836 - val_loss: 0.2745 - val_accuracy: 0.9405\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 21s 37ms/step - loss: 0.0415 - accuracy: 0.9860 - val_loss: 0.2880 - val_accuracy: 0.9422\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 20s 35ms/step - loss: 0.0366 - accuracy: 0.9863 - val_loss: 0.3019 - val_accuracy: 0.9424\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.0334 - accuracy: 0.9862 - val_loss: 0.3279 - val_accuracy: 0.9424\n",
      "CPU times: user 16min 14s, sys: 1min 56s, total: 18min 10s\n",
      "Wall time: 3min 33s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f80cfab0f98>"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "%%time\n",
    "model_n = Sequential()\n",
    "model_n.add(Dense(64, activation = 'relu', input_dim = 100000))\n",
    "model_n.add(Dense(1, activation ='sigmoid'))\n",
    "model_n.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model_n.fit_generator(generator = batch_generator(x_train_tfidf_norm, y_tr, 32),\n",
    "                      epochs = 10, \n",
    "                      validation_data = (x_validation_tfidf_norm, y_val),\n",
    "                      steps_per_epoch = x_train_tfidf_norm.shape[0]/32)"
   ]
  },
  {
   "source": [
    "## Using Dropout for Overfitting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 26s 44ms/step - loss: 0.5739 - accuracy: 0.7964 - val_loss: 0.3167 - val_accuracy: 0.9424\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.8738 - accuracy: 0.7787 - val_loss: 0.3830 - val_accuracy: 0.9424\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 22s 37ms/step - loss: 0.8162 - accuracy: 0.7830 - val_loss: 0.2810 - val_accuracy: 0.9424\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 24s 40ms/step - loss: 0.3484 - accuracy: 0.8593 - val_loss: 0.2562 - val_accuracy: 0.9430\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 22s 38ms/step - loss: 0.1655 - accuracy: 0.9331 - val_loss: 0.2608 - val_accuracy: 0.9422\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.1039 - accuracy: 0.9638 - val_loss: 0.2659 - val_accuracy: 0.9424\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 22s 37ms/step - loss: 0.0707 - accuracy: 0.9769 - val_loss: 0.2715 - val_accuracy: 0.9414\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.0531 - accuracy: 0.9822 - val_loss: 0.2824 - val_accuracy: 0.9403\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 20s 35ms/step - loss: 0.0453 - accuracy: 0.9837 - val_loss: 0.3016 - val_accuracy: 0.9419\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.0399 - accuracy: 0.9865 - val_loss: 0.3129 - val_accuracy: 0.9414\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f80cf965eb8>"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.fit_generator(generator=batch_generator(x_train_tfidf, y_tr, 32),\n",
    "                    epochs=10, validation_data=(x_validation_tfidf, y_val),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "source": [
    "## Shuffling Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator_shuffle(X_data, y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    np.random.shuffle(index)\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].toarray()\n",
    "        y_batch = y_data[y_data.index[index_batch]]\n",
    "        counter += 1\n",
    "        yield np.array(X_batch), np.array(y_batch)\n",
    "        if (counter > number_of_batches):\n",
    "            np.random.shuffle(index)\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 25s 40ms/step - loss: 0.3838 - accuracy: 0.9369 - val_loss: 0.1864 - val_accuracy: 0.9414\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 23s 39ms/step - loss: 0.1334 - accuracy: 0.9441 - val_loss: 0.1732 - val_accuracy: 0.9430\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 24s 42ms/step - loss: 0.0648 - accuracy: 0.9769 - val_loss: 0.1909 - val_accuracy: 0.9381\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 25s 43ms/step - loss: 0.0299 - accuracy: 0.9927 - val_loss: 0.2163 - val_accuracy: 0.9397\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 23s 39ms/step - loss: 0.0198 - accuracy: 0.9950 - val_loss: 0.2417 - val_accuracy: 0.9392\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 21s 36ms/step - loss: 0.0166 - accuracy: 0.9955 - val_loss: 0.2558 - val_accuracy: 0.9379\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 23s 40ms/step - loss: 0.0118 - accuracy: 0.9965 - val_loss: 0.2735 - val_accuracy: 0.9365\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 24s 41ms/step - loss: 0.0100 - accuracy: 0.9960 - val_loss: 0.2871 - val_accuracy: 0.9368\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 27s 47ms/step - loss: 0.0095 - accuracy: 0.9959 - val_loss: 0.3062 - val_accuracy: 0.9357\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 23s 40ms/step - loss: 0.0077 - accuracy: 0.9969 - val_loss: 0.3177 - val_accuracy: 0.9346\n",
      "CPU times: user 15min 44s, sys: 1min 52s, total: 17min 37s\n",
      "Wall time: 3min 58s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8013777cf8>"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "%%time\n",
    "model_s = Sequential()\n",
    "model_s.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model_s.add(Dense(1, activation='sigmoid'))\n",
    "model_s.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_s.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_tr, 32),\n",
    "                    epochs=10, validation_data=(x_validation_tfidf, y_val),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "source": [
    "## Shuffle and Dropout"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 29s 49ms/step - loss: 0.3737 - accuracy: 0.9375 - val_loss: 0.1844 - val_accuracy: 0.9416\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 23s 39ms/step - loss: 0.1436 - accuracy: 0.9445 - val_loss: 0.1730 - val_accuracy: 0.9430\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 24s 41ms/step - loss: 0.0717 - accuracy: 0.9730 - val_loss: 0.1895 - val_accuracy: 0.9392\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 24s 41ms/step - loss: 0.0349 - accuracy: 0.9903 - val_loss: 0.2123 - val_accuracy: 0.9381\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 23s 40ms/step - loss: 0.0222 - accuracy: 0.9943 - val_loss: 0.2310 - val_accuracy: 0.9370\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 24s 41ms/step - loss: 0.0169 - accuracy: 0.9946 - val_loss: 0.2546 - val_accuracy: 0.9370\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 23s 40ms/step - loss: 0.0132 - accuracy: 0.9957 - val_loss: 0.2672 - val_accuracy: 0.9368\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 23s 39ms/step - loss: 0.0110 - accuracy: 0.9955 - val_loss: 0.2842 - val_accuracy: 0.9373\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 23s 40ms/step - loss: 0.0098 - accuracy: 0.9962 - val_loss: 0.3014 - val_accuracy: 0.9373\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 22s 38ms/step - loss: 0.0082 - accuracy: 0.9970 - val_loss: 0.3092 - val_accuracy: 0.9354\n",
      "CPU times: user 16min 3s, sys: 1min 59s, total: 18min 2s\n",
      "Wall time: 3min 58s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7fbf528518>"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "%%time\n",
    "model_s_1 = Sequential()\n",
    "model_s_1.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model_s_1.add(Dropout(0.2))\n",
    "model_s_1.add(Dense(1, activation='sigmoid'))\n",
    "model_s_1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_s_1.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_tr, 32),\n",
    "                    epochs=10, validation_data=(x_validation_tfidf, y_val),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "source": [
    "## Learning Rate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 37s 62ms/step - loss: 0.2616 - accuracy: 0.9293 - val_loss: 0.1741 - val_accuracy: 0.9424\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 28s 48ms/step - loss: 0.0662 - accuracy: 0.9629 - val_loss: 0.2558 - val_accuracy: 0.9292\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 26s 45ms/step - loss: 0.0250 - accuracy: 0.9921 - val_loss: 0.2860 - val_accuracy: 0.9244\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 27s 47ms/step - loss: 0.0134 - accuracy: 0.9953 - val_loss: 0.3190 - val_accuracy: 0.9341\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 32s 54ms/step - loss: 0.0114 - accuracy: 0.9958 - val_loss: 0.3377 - val_accuracy: 0.9319\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 29s 51ms/step - loss: 0.0097 - accuracy: 0.9964 - val_loss: 0.3698 - val_accuracy: 0.9349\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 30s 51ms/step - loss: 0.0097 - accuracy: 0.9956 - val_loss: 0.3596 - val_accuracy: 0.9290\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 32s 54ms/step - loss: 0.0096 - accuracy: 0.9951 - val_loss: 0.3870 - val_accuracy: 0.9338\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 36s 62ms/step - loss: 0.0086 - accuracy: 0.9956 - val_loss: 0.3872 - val_accuracy: 0.9327\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 31s 54ms/step - loss: 0.0074 - accuracy: 0.9959 - val_loss: 0.3735 - val_accuracy: 0.9271\n",
      "CPU times: user 16min 32s, sys: 2min 8s, total: 18min 41s\n",
      "Wall time: 5min 8s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7efc4f9cf8>"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "custom_adam = keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model_testing_2 = Sequential()\n",
    "model_testing_2.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model_testing_2.add(Dense(1, activation='sigmoid'))\n",
    "model_testing_2.compile(optimizer=custom_adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_testing_2.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_tr, 32),\n",
    "                    epochs=10, validation_data=(x_validation_tfidf, y_val),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 41s 68ms/step - loss: 0.2273 - accuracy: 0.9414 - val_loss: 0.1879 - val_accuracy: 0.9424\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 31s 53ms/step - loss: 0.0512 - accuracy: 0.9808 - val_loss: 0.3134 - val_accuracy: 0.9338\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 28s 47ms/step - loss: 0.0187 - accuracy: 0.9926 - val_loss: 0.3171 - val_accuracy: 0.9322\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 28s 49ms/step - loss: 0.0140 - accuracy: 0.9937 - val_loss: 0.3170 - val_accuracy: 0.9190\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 28s 48ms/step - loss: 0.0110 - accuracy: 0.9949 - val_loss: 0.4019 - val_accuracy: 0.9384\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 29s 50ms/step - loss: 0.0084 - accuracy: 0.9960 - val_loss: 0.3401 - val_accuracy: 0.9252\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 29s 49ms/step - loss: 0.0072 - accuracy: 0.9965 - val_loss: 0.3432 - val_accuracy: 0.9247\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 28s 49ms/step - loss: 0.0068 - accuracy: 0.9973 - val_loss: 0.3478 - val_accuracy: 0.9163\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 29s 49ms/step - loss: 0.0072 - accuracy: 0.9969 - val_loss: 0.3670 - val_accuracy: 0.9131\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 28s 48ms/step - loss: 0.0074 - accuracy: 0.9969 - val_loss: 0.3785 - val_accuracy: 0.9252\n",
      "CPU times: user 16min 22s, sys: 2min 9s, total: 18min 31s\n",
      "Wall time: 4min 58s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7fbf53ae10>"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "%%time\n",
    "custom_adam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model_testing_3 = Sequential()\n",
    "model_testing_3.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model_testing_3.add(Dense(1, activation='sigmoid'))\n",
    "model_testing_3.compile(optimizer=custom_adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_testing_3.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_tr, 32),\n",
    "                    epochs=10, validation_data=(x_validation_tfidf, y_val),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 42s 70ms/step - loss: 0.2224 - accuracy: 0.9356 - val_loss: 0.2158 - val_accuracy: 0.9405\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 31s 53ms/step - loss: 0.0872 - accuracy: 0.9680 - val_loss: 0.3700 - val_accuracy: 0.9368\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 27s 46ms/step - loss: 0.0549 - accuracy: 0.9832 - val_loss: 0.5258 - val_accuracy: 0.9349\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 34s 58ms/step - loss: 0.0346 - accuracy: 0.9886 - val_loss: 0.3928 - val_accuracy: 0.9354\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 28s 47ms/step - loss: 0.0260 - accuracy: 0.9903 - val_loss: 0.4796 - val_accuracy: 0.9349\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 40s 68ms/step - loss: 0.0249 - accuracy: 0.9920 - val_loss: 0.5224 - val_accuracy: 0.9327\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 37s 63ms/step - loss: 0.0242 - accuracy: 0.9918 - val_loss: 0.5153 - val_accuracy: 0.9319\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 35s 61ms/step - loss: 0.0178 - accuracy: 0.9938 - val_loss: 0.4306 - val_accuracy: 0.9322\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 35s 60ms/step - loss: 0.0221 - accuracy: 0.9913 - val_loss: 0.4697 - val_accuracy: 0.9284\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 34s 58ms/step - loss: 0.0195 - accuracy: 0.9933 - val_loss: 0.3720 - val_accuracy: 0.9284\n",
      "CPU times: user 16min 38s, sys: 2min 21s, total: 19min\n",
      "Wall time: 5min 42s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7f004954a8>"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "%%time\n",
    "custom_adam = keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model_testing_4 = Sequential()\n",
    "model_testing_4.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model_testing_4.add(Dense(1, activation='sigmoid'))\n",
    "model_testing_4.compile(optimizer=custom_adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_testing_4.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_tr, 32),\n",
    "                              epochs=10, \n",
    "                              validation_data=(x_validation_tfidf, y_val),\n",
    "                              steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 59s 97ms/step - loss: 0.4546 - accuracy: 0.9351 - val_loss: 0.1989 - val_accuracy: 0.9424\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 36s 61ms/step - loss: 0.1671 - accuracy: 0.9451 - val_loss: 0.1752 - val_accuracy: 0.9430\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 44s 76ms/step - loss: 0.1145 - accuracy: 0.9497 - val_loss: 0.1758 - val_accuracy: 0.9435\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 31s 53ms/step - loss: 0.0658 - accuracy: 0.9778 - val_loss: 0.1864 - val_accuracy: 0.9400\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 33s 58ms/step - loss: 0.0375 - accuracy: 0.9899 - val_loss: 0.1997 - val_accuracy: 0.9384\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 34s 59ms/step - loss: 0.0262 - accuracy: 0.9928 - val_loss: 0.2136 - val_accuracy: 0.9376\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 38s 65ms/step - loss: 0.0166 - accuracy: 0.9957 - val_loss: 0.2261 - val_accuracy: 0.9376\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 34s 58ms/step - loss: 0.0135 - accuracy: 0.9963 - val_loss: 0.2392 - val_accuracy: 0.9381\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 34s 58ms/step - loss: 0.0115 - accuracy: 0.9961 - val_loss: 0.2509 - val_accuracy: 0.9376\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 36s 61ms/step - loss: 0.0101 - accuracy: 0.9962 - val_loss: 0.2630 - val_accuracy: 0.9362\n",
      "CPU times: user 17min 4s, sys: 2min 30s, total: 19min 34s\n",
      "Wall time: 6min 19s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7fb38cf898>"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "%%time\n",
    "custom_adam = keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model_testing_5 = Sequential()\n",
    "model_testing_5.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model_testing_5.add(Dense(1, activation='sigmoid'))\n",
    "model_testing_5.compile(optimizer=custom_adam,\n",
    "                        loss='binary_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "model_testing_5.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_tr, 32),\n",
    "                              epochs=10, \n",
    "                              validation_data=(x_validation_tfidf, y_val),\n",
    "                              steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "source": [
    "## Increasing number of hidden nodes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "580/580 [==============================] - 70s 113ms/step - loss: 0.3306 - accuracy: 0.9425 - val_loss: 0.1761 - val_accuracy: 0.9424\n",
      "Epoch 2/10\n",
      "580/580 [==============================] - 46s 79ms/step - loss: 0.1100 - accuracy: 0.9463 - val_loss: 0.1859 - val_accuracy: 0.9387\n",
      "Epoch 3/10\n",
      "580/580 [==============================] - 46s 79ms/step - loss: 0.0399 - accuracy: 0.9884 - val_loss: 0.2260 - val_accuracy: 0.9376\n",
      "Epoch 4/10\n",
      "580/580 [==============================] - 43s 74ms/step - loss: 0.0197 - accuracy: 0.9942 - val_loss: 0.2539 - val_accuracy: 0.9360\n",
      "Epoch 5/10\n",
      "580/580 [==============================] - 53s 92ms/step - loss: 0.0135 - accuracy: 0.9959 - val_loss: 0.2730 - val_accuracy: 0.9357\n",
      "Epoch 6/10\n",
      "580/580 [==============================] - 64s 110ms/step - loss: 0.0100 - accuracy: 0.9966 - val_loss: 0.2904 - val_accuracy: 0.9341\n",
      "Epoch 7/10\n",
      "580/580 [==============================] - 60s 103ms/step - loss: 0.0082 - accuracy: 0.9963 - val_loss: 0.3066 - val_accuracy: 0.9352\n",
      "Epoch 8/10\n",
      "580/580 [==============================] - 65s 112ms/step - loss: 0.0093 - accuracy: 0.9962 - val_loss: 0.3224 - val_accuracy: 0.9352\n",
      "Epoch 9/10\n",
      "580/580 [==============================] - 58s 100ms/step - loss: 0.0078 - accuracy: 0.9963 - val_loss: 0.3255 - val_accuracy: 0.9322\n",
      "Epoch 10/10\n",
      "580/580 [==============================] - 51s 88ms/step - loss: 0.0064 - accuracy: 0.9973 - val_loss: 0.3399 - val_accuracy: 0.9322\n",
      "CPU times: user 29min 22s, sys: 2min 55s, total: 32min 18s\n",
      "Wall time: 9min 18s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7ef5128cc0>"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "%%time\n",
    "model_s_2 = Sequential()\n",
    "model_s_2.add(Dense(128, activation='relu', input_dim=100000))\n",
    "model_s_2.add(Dense(1, activation='sigmoid'))\n",
    "model_s_2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_s_2.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_tr, 32),\n",
    "                        epochs=10, \n",
    "                        validation_data=(x_validation_tfidf, y_val),\n",
    "                        steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}